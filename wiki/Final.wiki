CS467 Team4

<b>Problem Overview</b>

We augmented the problem statement by adding the ability to have the robot be completely autonomous during Task Mode. Instead of using Subversion through Runestone we decided to use Runestone wiki page for official news postings and important links. Instead we used Google Code for our documents because it was a little easier to access, and didn’t require a log-in on every access. Google Code has Subversion, a Wiki, a calendar for scheduling meetings, and it also gave us the option to store document in all formats.

Milestones and Deliverables

1.	We set up our Wiki page, providing, names, contact information, and login names for each group member. We provided a section for our Milestone logs and also our international group meeting logs. We had an Agenda section so that the entire group stayed on same page. Included in the page was many links to resources, project guidelines, and communication tools that were needed. We set up a group format to have 2 leaders, 1 in U.S. and 1 in Sweden. Doing this would prevent confusion amongst all members, but instead the two leaders would be the voice for their country. We tracked project progress through a Gantt Scheme program. The one thing we failed to do for this milestone was the tracking of individual contributions.
2.	Two sub teams were decided on for working and dividing the responsibilities for the project. The U.S. team would handle the Robot software and hardware part of the project, leaving the Uppsala team to work with the GUI and server part. The main decisions were included in the design document. We decided our special features would be color decoding of an object, which would be computed by the robot CPU, and path finding, which would be computed within the sever unit CPU). By milestone 2, the robot was built, and all ideas were finally integrated into our completed design document. Some things we missed within the document were specifications regarding path finding, and the specifics about our actual goals of the project.  We also didn’t include application level packet format for data exchange between the robot and server, and sever and client. 
3.	The workload page had been created and updated by all members. The robot had been designed, constructed, and flashed with lejos. Connection between server and GUI client had been established.  Video streaming was complete using RTP-packets for the video and UDP for the protocol to travel through the internet. We used TCP to receive and send messages to and from the client. The GUI was functional and could connect to the sever and relay messages back and forth, and receive display a video feed. By this milestone, we still needed to connect the robot and client to the server simultaneously. Path finding on the server end still needed to be implemented. Basic communication, motor skills, and sensory response needed to be completed. The algorithm to search the environment for objects in autonomous mode still had to be implemented. The outline of the states for the robot’s behavior needed to be translated into code.
4.	The implementation to handle sensor input had been established, but the actions taken by the input is directly related to the states which were still in development. We also had the starting writing a robot class, that held all information for the robot and included the methods that handle the movement and a stack of previous moves. This class also contains a map of the environment that the robot will create as it explores. This all gets tied into the RobotStates class which establishes connection, checks the connection, and handles messages from the server.  We decided to use the function rotateTo() for motor movement which rotates the motor to a certain degree. There is another way to implement movement where we tell it to move and then use time as a metric. We thought using the rotateTo() method would help against the inconsistencies in the motors due to battery power. Picture processing method was completed and integrated with the server. The method returns a map of an array containing zeros and ones, giving a picture where the ones represent obstacles. The picture is provided by another method, which captures a picture of the environment before the server commences communication with the GUI. The server has been equipped with a path finding algorithm that finds a path, not necessarily the next path. This path finding happens between two positions in a multiple array, avoiding obstacles in shapes of one’s. The picture used in the picture processing is captured before the robot enters the environment. The server and the robot communication still had to be implemented and tested. The robot needed to know how much distance a cell in the path finding map represents in real life. We still needed to determine the fixed point camera position and angle, and other environment parameters. Then we had to decide on how to form the navigation instructions that was built from the algorithm. The threshold needed to be adjusted for the picture processing. We still needed to implement the code of the created pseudo code for the path finding algorithm in task mode. We also needed to complete the implementation of the states, and integrating the sensory responses which correspond to each state. We also needed to integrate the U.S. and Sweden parts and create the flow of data through the server from the GUI client to the robot. This included the processing or reformatting of some data before it is relayed to the robot. We still needed to calibrate the robot movement against the image coordinate system, and check what the offset is on the image for a given robot distance instruction when starting up

Production Process

Reason for Achieving

Reasons for Failing